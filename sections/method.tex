\section{Deinterleaving Methods}
	In the deinterleaving problem, given $\{v(t)\}_{t=1}^T$ we are
        interested in inferring $\{u(t)\}_{t=1}^T$.  In other words,
        we want to find the users who initiated each request from the
        sequence generated by the interleaving process described in
        Section \ref{subsec:interle}.
	
	We present two candidate approaches for inference.  One is
        based on reducing the interleaving process to an AHMM as
        discussed in Section \ref{subsec:interle}.  This approach has
        been used for deinterleaving of Markov chains with small
        number of chains (users) and state space
        \cite{minot2014separation}.  Next, we propose to deinterleave
        using an RNN and an LSTM model which have recently been shown to 
        perform well in many time-series analysis tasks\cite{chung2014empirical,Hochreiter}.
	
	\subsection{Inference on Augmented HMM}
        We can model the whole interleaving process as an AHMM
        and use learning techniques (like
        EM) to learn its parameters and use Viterbi inference to
        determine the most probable hidden (augmented) states $h(t)$
        from which we can extract the most probable user $u(t)$.  The
        main difficulty of applying this framework is that the state
        space of hidden variable, Figure \ref{fig:hsmm}, is very
        large.  More specifically, there are $m (nq)^m$ possible
        states of $h$ and as we increase number of webpages $n$ or
        users $m$ the state space grows exponentially.  The huge state
        space, makes the inference and learning very hard and as we
        show in Section \ref{subsec:viterbi} for synthetic
        experiments, even when the model parameters are known,
        deinterleaving performs (using Viterbi coding) poorly.
	
	\subsection{Inference using an RNN}
        RNNs \cite{lecun2015deep} are popular for modeling time
        series data. Given the input $v_t$ and hidden state $h_{t-1}$,
        the RNN computes the next hidden state representation $h_t$
        and output $u_t$ using the following recurrent relationships
	\begin{align}
	\label{eq:rnn}
	h_{t} &= f(W_v v_{t} + W_h h_{t - 1} + b)\\
	u_t &= W_u h_t
	\end{align}
	%
	where $W_v$, $W_h$, $W_v$ and $b$ are the network parameters, and $f()$ is some non-linear function. 
        An example of $f$ could be a sigmoid $f(z) = \sigma(z) = 1/(1+\exp(-z))$ or rectified linear unit $f(z) = \max(0,z)$. 
	
	
	For our specific problem of deinterleaving, an RNN can 
        by posed as a multi-class classification problem, where
        the input is the observed webpage and the output will be the
        identified user who requested that webpage. Specifically, each
        data instantiation consists of a sequence of user-request
        pairs, i.e., $(u(t), v(t))$.  This represents who was the user
        at a given time $t$ and what request was produced by that
        user. Both the user and the request are represented by an
        integer. The RNN is unrolled for the entire length of one
        sequence. The users and request integers are converted to
        one-hot encoding to enable learning. Thus if there are $b$
        possible web pages, the requests become $b$-dimensional
        vectors and for $m$ users, it becomes an $m$-dimensional
        vector. The request vectors are fed as input to the RNN model,
        while the output is the corresponding user at each
        time-step. The RNNâ€™s $m$-dimensional output is passed through
        a softmax layer to convert it into probabilities and the user
        with higher probability is compared against the ground
        truth. Performance is measured in terms of accurately
        identifying the user at each instant.
	
	A common approach to solving the optimization problem is to
        randomly initialize network parameters $W_v, W_h, W_u$ and
        apply stochastic gradient descent (SGD) (for RNNs, it is also
        referred to as a Backpropagation-through-time (BPTT)
        algorithm). In particular, we use a variation of the standard
        SGD called Adam \cite{kingma2014adam}, which allows for
        adaptive learning rates using the past gradients, similar to
        using momentum. This results in faster convergence compared to
        other adaptive algorithms.
	
	
	
